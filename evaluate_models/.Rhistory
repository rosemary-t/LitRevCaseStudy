}
}
if("cos_moy"%in%x){
if(sum("sin_moy"%in%x)==0){
x <- c(x,"sin_moy")
}
}
return(x)
}
func_pairhours <- function(x){
if("sin_hod"%in%x){
if(sum("cos_hod"%in%x)==0){
x <- c(x,"cos_hod")
}
}
if("cos_hod"%in%x){
if(sum("sin_hod"%in%x)==0){
x <- c(x,"sin_hod")
}
}
return(x)
}
##############################################################################################
## Find optimal GBM parameters by cross validation on the training data
## First, get feature importances
## by training some really sparse regression trees to get feature importances for three key quantiles
vars <- list()
for(j in c(0.1,0.5,0.9)){
mod <- gbm(formula = formula2,
distribution = list(name = "quantile", alpha = j),
data = training_data,
n.trees = 1000L,
interaction.depth = 2,
shrinkage = 0.01,
n.minobsinnode=30, # minimum number obs per leaf
bag.fraction = 0.9, # fraction of obervations used for next tree
keep.data =F,
verbose=F)
tmp <- data.table(summary(mod,n.trees = 1000L, plotit = F))
vars[[paste0("q",j)]] <- as.character(tmp[rel.inf>=1]$var)
rm(tmp,mod)
}
unique_vars <- unique(unlist(vars)) # get unique variables at each location
unique_vars <- func_pairseason(unique_vars) # add in the other sin/cos moy term if only one in unique_vars already
unique_vars <- func_pairhours(unique_vars)  # same for sin/cos hod
final_formula <- as.formula(paste0("TARGETVAR ~ ",paste0(unique_vars,collapse = " + ")))
var_mono <- rep(0,length(unique_vars))
var_mono[which(unique_vars %in% c("sin_moy","cos_moy","sin_hod","cos_hod"))] <- -1 # set var_monotone to -1 for the circular (sin/cos) terms
## find optimal shirnkage and interaction depth (~50 mins)
modelparams <- find_opt_params(training_data, final_formula, var_mono,
searchGridSubCol$shrinkage, searchGridSubCol$interaction.depth, quantiles=q_list)
## RUN UP TO HERE
## now fit on numbered folds (training) and generate forecasts for test fold, using optimal parameters
gbm_mqr <- MQR_gbm(data = zdata,
formula = final_formula,
quantiles = q_list,
gbm_params = list(interaction.depth = modelparams$inter,
n.trees = 1000,
shrinkage = modelparams$shrink,
n.minobsinnode = 30,
bag.fraction = 0.5,
keep.data = F,
var.monotone = var_mono),
parallel = T,
cores = detectCores(),
Sort=T)
gbm_mqr <- as.data.table(gbm_mqr)
forecasts <- cbind(zdata[,c("timestamp", "kfold")], gbm_mqr)
save(forecasts, file=paste0(path, zone, "final_fcs.Rda"))
print (modelparams)
print (Sys.time() - start_time)
zones <- c(2:9)
save(modelparams, file=paste0(path, zone, "modelparams.Rda"))
fwrite(modelparams, file=paste0(path, zone, "modelparams.csv"))
require(data.table)
require(graphics)
require(ProbCast)
"Uses features deemed important from the q50 feature importance,
plus all the zones' q50 forecasts, and does feature importance again on these."
## need to load all the data
path <- "C:\\Users\\rosemaryt\\ShareFile\\Personal Folders\\reproducing short term forecasting papers\\day ahead forecasts_v2\\zone"
zones <- c(2:9)
# zone <- 1
q_list <- seq(0.05, 0.95,0.05) # the quantiles we want to forecast for
##############################################################################################
########################################## functions #########################################
##############################################################################################
find_opt_params <- function(input_data,fmla, mono_var, shrinkage_vec,interaction_vec,
quantiles, n_trees = 1000L,ncores= NULL,...){
## find the optimal shrinkage and interation depth.
## input data must be already WITHOUT the fold you finally want forecasts for...
if(length(shrinkage_vec)!=length(interaction_vec)){
stop("length(shrinkage_vec))!=length(interaction_vec")
}
if(is.null(ncores)){ncores <- length(quantiles)}
mqr_cv <- lapply(seq_along(shrinkage_vec),function(x){
preds <- MQR_gbm(data = input_data,
formula = fmla,
quantiles = quantiles,
gbm_params = list(shrinkage = shrinkage_vec[x],
interaction.depth = interaction_vec[x],
n.trees = n_trees,
var.monotone=mono_var),
parallel = T,
cores = ncores,
pckgs = c("data.table"),
perf.plot=F,
Sort = T,
SortLimits = list(U=1, L=0),
pred_ntree = n_trees)
rel <- data.table(reliability(qrdata = preds, realisations = input_data[,get(as.character(fmla[[2]]))],plot.it = F))
pball <- data.table(pinball(qrdata = preds, realisations = input_data[,get(as.character(fmla[[2]]))],plot.it = F))
return(list(shrink = shrinkage_vec[x],
inter = interaction_vec[x],
reliab = rel,
pinb = pball))
})
cv_results <- lapply(mqr_cv,as.data.table)
cv_results <- rbindlist(cv_results)
# average pinball loss for each run and price region
cv_avpb <- cv_results[,list(av_pb=mean(pinb.Loss)),by=.(shrink,inter)]
# find minimum shrinkage and depth combo
cv_avpb <- cv_avpb[,.SD[which.min(av_pb)]] # just keep the row with the best (average) pinball loss
return (cv_avpb)
}
## make sure both sin and cos terms are included if just one is (both either hod or moy):
func_pairseason <- function(x){
if("sin_moy"%in%x){
if(sum("cos_moy"%in%x)==0){
x <- c(x,"cos_moy")
}
}
if("cos_moy"%in%x){
if(sum("sin_moy"%in%x)==0){
x <- c(x,"sin_moy")
}
}
return(x)
}
func_pairhours <- function(x){
if("sin_hod"%in%x){
if(sum("cos_hod"%in%x)==0){
x <- c(x,"cos_hod")
}
}
if("cos_hod"%in%x){
if(sum("sin_hod"%in%x)==0){
x <- c(x,"sin_hod")
}
}
return(x)
}
##############################################################################################
for (zone in zones){
print (zone)
zstart_time <- Sys.time()
## parameter options to explore
searchGridSubCol <- expand.grid(shrinkage = seq(0.01, 0.1, 0.03),interaction.depth = seq(2, 12, 3))
n_param_combs <- dim(searchGridSubCol)[1] # number of parameter combinations we need to iterate over.
## load formula and var_mono
load(paste0(path, zone, "fcinputs.rda"))
## load training data
load(paste0(path, zone, "traindata.rda"))
## add on all the q50 forecasts
load(paste0(path, "s_q50_fcs.Rda"))
zdata <- merge(zdata, forecast_df, by=c('timestamp', 'kfold'))
## and remember to add them into FORMULA too...
not_fc_inputs <- c("timestamp", "kfold") # specify what columns in q50 forecasts are NOT forecast inputs
q50_inputs <- setdiff(names(forecast_df), not_fc_inputs)
existing_inputs <- labels(terms(FORMULA))
all_inputs <- c(existing_inputs, q50_inputs)
formula2 <- as.formula(paste0("TARGETVAR ~ ",paste0(all_inputs,collapse = " + ")))
## if we want to select from all possible forecast inputs (rather than using only those we found via feature selection in 1st stage)
# not_fc_inputs <- c("ZONEID", "TARGETVAR", "timestamp", "kfold") # easier to specify what columns are NOT forecast inputs!
# fc_inputs <- setdiff(names(training_data), not_fc_inputs)
# formula_new <- as.formula(paste0("TARGETVAR ~ ",paste0(fc_inputs,collapse = " + ")))
training_data <- zdata[kfold != 'Test']
## Find optimal GBM parameters by cross validation on the training data
## First, get feature importances
## by training some really sparse regression trees to get feature importances for three key quantiles
vars <- list()
for(j in c(0.1,0.5,0.9)){
mod <- gbm(formula = formula2,
distribution = list(name = "quantile", alpha = j),
data = training_data,
n.trees = 1000L,
interaction.depth = 2,
shrinkage = 0.01,
n.minobsinnode=30, # minimum number obs per leaf
bag.fraction = 0.9, # fraction of obervations used for next tree
keep.data =F,
verbose=F)
tmp <- data.table(summary(mod,n.trees = 1000L, plotit = F))
vars[[paste0("q",j)]] <- as.character(tmp[rel.inf>=1]$var)
rm(tmp,mod)
}
unique_vars <- unique(unlist(vars)) # get unique variables at each location
unique_vars <- func_pairseason(unique_vars) # add in the other sin/cos moy term if only one in unique_vars already
unique_vars <- func_pairhours(unique_vars)  # same for sin/cos hod
final_formula <- as.formula(paste0("TARGETVAR ~ ",paste0(unique_vars,collapse = " + ")))
var_mono <- rep(0,length(unique_vars))
var_mono[which(unique_vars %in% c("sin_moy","cos_moy","sin_hod","cos_hod"))] <- -1 # set var_monotone to -1 for the circular (sin/cos) terms
## find optimal shirnkage and interaction depth (~50 mins)
modelparams <- find_opt_params(training_data, final_formula, var_mono,
searchGridSubCol$shrinkage, searchGridSubCol$interaction.depth, quantiles=q_list)
## RUN UP TO HERE
## now fit on numbered folds (training) and generate forecasts for test fold, using optimal parameters
gbm_mqr <- MQR_gbm(data = zdata,
formula = final_formula,
quantiles = q_list,
gbm_params = list(interaction.depth = modelparams$inter,
n.trees = 1000,
shrinkage = modelparams$shrink,
n.minobsinnode = 30,
bag.fraction = 0.5,
keep.data = F,
var.monotone = var_mono),
parallel = T,
cores = detectCores(),
Sort=T)
gbm_mqr <- as.data.table(gbm_mqr)
forecasts <- cbind(zdata[,c("timestamp", "kfold")], gbm_mqr)
save(forecasts, file=paste0(path, zone, "final_fcs.Rda"))
fwrite(modelparams, file=paste0(path, zone, "modelparams.csv"))
#print (Sys.time() - zstart_time)
}
# zone <- 1
short_qs <- seq(0.1, 0.9, 0.1)
require(data.table)
require(graphics)
require(ProbCast)
"Uses features deemed important from the q50 feature importance,
plus all the zones' q50 forecasts, and does feature importance again on these."
## need to load all the data
path <- "C:\\Users\\rosemaryt\\ShareFile\\Personal Folders\\reproducing short term forecasting papers\\day ahead forecasts_v2\\zone"
zones <- c(1:10)
# zone <- 1
short_qs <- seq(0.1, 0.9, 0.1)
q_list <- seq(0.05, 0.95,0.05) # the quantiles we want to forecast for
##############################################################################################
########################################## functions #########################################
##############################################################################################
find_opt_params <- function(input_data,fmla, mono_var, shrinkage_vec,interaction_vec,
quantiles, n_trees = 1000L,ncores= NULL,...){
## find the optimal shrinkage and interation depth.
## input data must be already WITHOUT the fold you finally want forecasts for...
if(length(shrinkage_vec)!=length(interaction_vec)){
stop("length(shrinkage_vec))!=length(interaction_vec")
}
if(is.null(ncores)){ncores <- length(quantiles)}
mqr_cv <- lapply(seq_along(shrinkage_vec),function(x){
preds <- MQR_gbm(data = input_data,
formula = fmla,
quantiles = quantiles,
gbm_params = list(shrinkage = shrinkage_vec[x],
interaction.depth = interaction_vec[x],
n.trees = n_trees,
var.monotone=mono_var),
parallel = T,
cores = ncores,
pckgs = c("data.table"),
perf.plot=F,
Sort = T,
SortLimits = list(U=1, L=0),
pred_ntree = n_trees)
rel <- data.table(reliability(qrdata = preds, realisations = input_data[,get(as.character(fmla[[2]]))],plot.it = F))
pball <- data.table(pinball(qrdata = preds, realisations = input_data[,get(as.character(fmla[[2]]))],plot.it = F))
return(list(shrink = shrinkage_vec[x],
inter = interaction_vec[x],
reliab = rel,
pinb = pball))
})
cv_results <- lapply(mqr_cv,as.data.table)
cv_results <- rbindlist(cv_results)
# average pinball loss for each run and price region
cv_avpb <- cv_results[,list(av_pb=mean(pinb.Loss)),by=.(shrink,inter)]
# find minimum shrinkage and depth combo
cv_avpb <- cv_avpb[,.SD[which.min(av_pb)]] # just keep the row with the best (average) pinball loss
return (cv_avpb)
}
## make sure both sin and cos terms are included if just one is (both either hod or moy):
func_pairseason <- function(x){
if("sin_moy"%in%x){
if(sum("cos_moy"%in%x)==0){
x <- c(x,"cos_moy")
}
}
if("cos_moy"%in%x){
if(sum("sin_moy"%in%x)==0){
x <- c(x,"sin_moy")
}
}
return(x)
}
func_pairhours <- function(x){
if("sin_hod"%in%x){
if(sum("cos_hod"%in%x)==0){
x <- c(x,"cos_hod")
}
}
if("cos_hod"%in%x){
if(sum("sin_hod"%in%x)==0){
x <- c(x,"sin_hod")
}
}
return(x)
}
##############################################################################################
for (zone in zones){
print (zone)
zstart_time <- Sys.time()
## parameter options to explore
searchGridSubCol <- expand.grid(shrinkage = seq(0.01, 0.1, 0.03),interaction.depth = seq(2, 12, 3))
n_param_combs <- dim(searchGridSubCol)[1] # number of parameter combinations we need to iterate over.
## load formula and var_mono
load(paste0(path, zone, "fcinputs.rda"))
## load training data
load(paste0(path, zone, "traindata.rda"))
## add on all the q50 forecasts
load(paste0(path, "s_q50_fcs.Rda"))
zdata <- merge(zdata, forecast_df, by=c('timestamp', 'kfold'))
## and remember to add them into FORMULA too...
not_fc_inputs <- c("timestamp", "kfold") # specify what columns in q50 forecasts are NOT forecast inputs
q50_inputs <- setdiff(names(forecast_df), not_fc_inputs)
existing_inputs <- labels(terms(FORMULA))
all_inputs <- c(existing_inputs, q50_inputs)
formula2 <- as.formula(paste0("TARGETVAR ~ ",paste0(all_inputs,collapse = " + ")))
## if we want to select from all possible forecast inputs (rather than using only those we found via feature selection in 1st stage)
# not_fc_inputs <- c("ZONEID", "TARGETVAR", "timestamp", "kfold") # easier to specify what columns are NOT forecast inputs!
# fc_inputs <- setdiff(names(training_data), not_fc_inputs)
# formula_new <- as.formula(paste0("TARGETVAR ~ ",paste0(fc_inputs,collapse = " + ")))
training_data <- zdata[kfold != 'Test']
## Find optimal GBM parameters by cross validation on the training data
## First, get feature importances
## by training some really sparse regression trees to get feature importances for three key quantiles
vars <- list()
for(j in c(0.1,0.5,0.9)){
mod <- gbm(formula = formula2,
distribution = list(name = "quantile", alpha = j),
data = training_data,
n.trees = 1000L,
interaction.depth = 2,
shrinkage = 0.01,
n.minobsinnode=30, # minimum number obs per leaf
bag.fraction = 0.9, # fraction of obervations used for next tree
keep.data =F,
verbose=F)
tmp <- data.table(summary(mod,n.trees = 1000L, plotit = F))
vars[[paste0("q",j)]] <- as.character(tmp[rel.inf>=1]$var)
rm(tmp,mod)
}
unique_vars <- unique(unlist(vars)) # get unique variables at each location
unique_vars <- func_pairseason(unique_vars) # add in the other sin/cos moy term if only one in unique_vars already
unique_vars <- func_pairhours(unique_vars)  # same for sin/cos hod
final_formula <- as.formula(paste0("TARGETVAR ~ ",paste0(unique_vars,collapse = " + ")))
var_mono <- rep(0,length(unique_vars))
var_mono[which(unique_vars %in% c("sin_moy","cos_moy","sin_hod","cos_hod"))] <- -1 # set var_monotone to -1 for the circular (sin/cos) terms
## find optimal shirnkage and interaction depth (~50 mins)
modelparams <- find_opt_params(training_data, final_formula, var_mono,
searchGridSubCol$shrinkage, searchGridSubCol$interaction.depth, quantiles=short_qs)
## now fit on numbered folds (training) and generate forecasts for test fold, using optimal parameters
gbm_mqr <- MQR_gbm(data = zdata,
formula = final_formula,
quantiles = q_list,
gbm_params = list(interaction.depth = modelparams$inter,
n.trees = 1000,
shrinkage = modelparams$shrink,
n.minobsinnode = 30,
bag.fraction = 0.5,
keep.data = F,
var.monotone = var_mono),
parallel = T,
cores = detectCores(),
Sort=T)
gbm_mqr <- as.data.table(gbm_mqr)
forecasts <- cbind(zdata[,c("timestamp", "kfold")], gbm_mqr)
save(forecasts, file=paste0(path, zone, "final_fcs.Rda"))
fwrite(modelparams, file=paste0(path, zone, "modelparams.csv"))
#print (Sys.time() - zstart_time)
}
require(data.table)
require(ProbCast)
require(rstudioapi)
setwd(dirname(getActiveDocumentContext()$path))
zones <- c(1:10)
models <- c("Persistence", "VAR", "MC", "EMD")
Nboots <- 1000 # number of times to bootstrap resample
## load persistence model forecasts
persistence_quantiles_list <- vector(mode = "list", length = length(zones))
persistence_details_list <- vector(mode = "list", length = length(zones))
for (i in zones){
load(file=paste0("../persistence/zone", i, "_forecasts.rda"))
persistence_fcs[[2]][, id := c(1:dim(persistence_fcs[[2]])[1])] # add an row index count
persistence_quantiles_list[[i]] <- persistence_fcs[[1]]
persistence_details_list[[i]] <- persistence_fcs[[2]]
if (i == 1){
## initialise a record of the (issue_time, target_time) pairs that exist for ALL zones
persistence_times_mutual <- persistence_fcs[[2]][,.(issue_time, target_time, Horizon)]
}else{
zone_rows <- persistence_fcs[[2]][,.(issue_time, target_time, Horizon)]
persistence_times_mutual <- merge(persistence_times_mutual, zone_rows) # keep the rows from mutual_rows, that also appear in zone_rows
}
# print (dim(persistence_times_mutual))
}
horizons <- unique(persistence_details_list[[1]]$Horizon)
## load forecasts from each other model too
VAR_quantiles_list <- vector(mode = "list", length = length(zones))
VAR_details_list <- vector(mode = "list", length = length(zones))
MC_quantiles_list <- vector(mode = "list", length = length(zones))
MC_details_list <- vector(mode = "list", length = length(zones))
EMD_quantiles_list <- vector(mode = "list", length = length(zones))
EMD_details_list <- vector(mode = "list", length = length(zones))
for (i in zones){
load(file=paste0("../VAR/final_quantiles_zone",i,".rda"))
VAR_fcs[[2]][, id := c(1:dim(VAR_fcs[[2]])[1])] # add a row index count
VAR_quantiles_list[[i]] <- VAR_fcs[[1]]
VAR_details_list[[i]] <- VAR_fcs[[2]]
load(file=paste0("../MC/test_forecasts_zone",i,".rda"))
MC_fcs[[2]][, id := c(1:dim(MC_fcs[[2]])[1])] # add an row index count
MC_quantiles_list[[i]] <- MC_fcs[[1]]
MC_details_list[[i]] <- MC_fcs[[2]]
load(file=paste0("../EMD/final_quantiles_zone",i,".rda"))
EMD_fcs[[2]][, id := c(1:dim(EMD_fcs[[2]])[1])] # add an row index count
EMD_quantiles_list[[i]] <- EMD_fcs[[1]]
EMD_details_list[[i]] <- EMD_fcs[[2]]
if (i == 1){
## initialise a record of the (issue_time, target_time) pairs that exist for ALL zones
VAR_times_mutual <- VAR_fcs[[2]][,.(issue_time, target_time, Horizon)]
MC_times_mutual <- MC_fcs[[2]][,.(issue_time, target_time, Horizon)]
EMD_times_mutual <- EMD_fcs[[2]][,.(issue_time, target_time, Horizon)]
}else{
varzone_rows <- VAR_fcs[[2]][,.(issue_time, target_time, Horizon)]
VAR_times_mutual <- merge(VAR_times_mutual, varzone_rows) # keep the rows from mutual_rows, that also appear in zone_rows
mczone_rows <- MC_fcs[[2]][,.(issue_time, target_time, Horizon)]
MC_times_mutual <- merge(MC_times_mutual, mczone_rows)
emdzone_rows <- VAR_fcs[[2]][,.(issue_time, target_time, Horizon)]
EMD_times_mutual <- merge(EMD_times_mutual, emdzone_rows)
}
}
rm(VAR_fcs)
rm(MC_fcs)
rm(EMD_fcs)
# now get the (issue_time, target_time) pairs we have forecasts for all models at, for all zones
mutual_times <- merge(merge(persistence_times_mutual, VAR_times_mutual), merge(MC_times_mutual, EMD_times_mutual))
model_errors <- CJ(Horizon=horizons, model=models)
## we are ready to bootstrap.
for (h in horizons){
print (h)
horizon_times <- mutual_times[Horizon==h, target_time]
p_h_errs <- matrix(nrow=Nboots, ncol=3)
var_h_errs <- matrix(nrow=Nboots, ncol=3)
mc_h_errs <- matrix(nrow=Nboots, ncol=3)
emd_h_errs <- matrix(nrow=Nboots, ncol=3)
for (nb in c(1:Nboots)){
sample_times <- sample(horizon_times, length(horizon_times), replace=TRUE)
p_errs <- matrix(nrow=length(zones), ncol=3) # 1st col=MAE, 2nd col=RMSE, 3rd col=pinball.
var_errs <- matrix(nrow=length(zones), ncol=3)
mc_errs <- matrix(nrow=length(zones), ncol=3)
emd_errs <- matrix(nrow=length(zones), ncol=3)
## now calculate errors for each zone and save in relevant matrices
for (z in zones){
## first, persistence
persistence_h_details <- persistence_details_list[[z]][Horizon==h]
persistence_short_indices <- match(sample_times, persistence_h_details[, target_time])
persistence_indices <- persistence_h_details[persistence_short_indices, id]
p_errors <- persistence_quantiles_list[[z]][persistence_indices, q50] - persistence_details_list[[z]][persistence_indices, ActualPower]
p_errs[z, 1] <- mean(abs(p_errors))
p_errs[z, 2] <- sqrt(mean(p_errors^2))
p_pinball <- pinball(persistence_quantiles_list[[z]][persistence_indices], realisations = persistence_details_list[[z]][persistence_indices, ActualPower], plot.it=FALSE)
p_errs[z, 3] <- mean(p_pinball$Loss)
## now, VAR
var_h_details <- VAR_details_list[[z]][Horizon==h]
var_short_indices <- match(sample_times, var_h_details[, target_time])
var_indices <- var_h_details[var_short_indices, id]
var_errors <- VAR_quantiles_list[[z]][var_indices, q50] - VAR_details_list[[z]][var_indices, ActualPower]
var_errs[z, 1] <- mean(abs(var_errors), na.rm=TRUE)
var_errs[z, 2] <- sqrt(mean(var_errors^2, na.rm=TRUE))
var_pinball <- pinball(VAR_quantiles_list[[z]][var_indices], realisations = VAR_details_list[[z]][var_indices, ActualPower], plot.it=FALSE)
var_errs[z, 3] <- mean(var_pinball$Loss)
## MC
mc_h_details <- MC_details_list[[z]][Horizon==h]
mc_short_indices <- match(sample_times, mc_h_details[, target_time])
mc_indices <- mc_h_details[mc_short_indices, id]
mc_errors <- MC_quantiles_list[[z]][mc_indices, q50] - MC_details_list[[z]][mc_indices, ActualPower]
mc_errs[z, 1] <- mean(abs(mc_errors), na.rm=TRUE)
mc_errs[z, 2] <- sqrt(mean(mc_errors^2, na.rm=TRUE))
mc_pinball <- pinball(MC_quantiles_list[[z]][mc_indices], realisations = MC_details_list[[z]][mc_indices, ActualPower], plot.it=FALSE)
mc_errs[z, 3] <- mean(mc_pinball$Loss)
## and EMD.
emd_h_details <- EMD_details_list[[z]][Horizon==h]
emd_short_indices <- match(sample_times, emd_h_details[, target_time])
emd_indices <- emd_h_details[emd_short_indices, id]
emd_errors <- EMD_quantiles_list[[z]][emd_indices, q50] - EMD_details_list[[z]][emd_indices, ActualPower]
emd_errs[z, 1] <- mean(abs(emd_errors), na.rm=TRUE)
emd_errs[z, 2] <- sqrt(mean(emd_errors^2, na.rm=TRUE))
emd_pinball <- pinball(EMD_quantiles_list[[z]][emd_indices], realisations = EMD_details_list[[z]][emd_indices, ActualPower], plot.it=FALSE)
emd_errs[z, 3] <- mean(emd_pinball$Loss)
}
p_h_errs[nb,] <- colMeans(p_errs)
var_h_errs[nb,] <- colMeans(var_errs)
mc_h_errs[nb,] <- colMeans(mc_errs)
emd_h_errs[nb,] <- colMeans(emd_errs)
}
cols <- c("MAE", "RMSE", "Pinball")
model_errors[(Horizon==h) & (model=="Persistence"), (cols) := as.list(colMeans(p_h_errs))]
model_errors[(Horizon==h) & (model=="VAR"), (cols) := as.list(colMeans(var_h_errs))]
model_errors[(Horizon==h) & (model=="MC"), (cols) := as.list(colMeans(mc_h_errs))]
model_errors[(Horizon==h) & (model=="EMD"), (cols) := as.list(colMeans(emd_h_errs))]
}
# fwrite(model_errors, file=paste0("./all_model_errors.csv"))
# model_errors <- fread(file=paste0("./all_model_errors.csv"))
fwrite(model_errors, file=paste0("./all_model_errors.csv"))
