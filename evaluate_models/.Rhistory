}
require(data.table)
require(graphics)
require(ProbCast)
"Uses features deemed important from the q50 feature importance,
plus all the zones' q50 forecasts, and does feature importance again on these."
start_time <- Sys.time()
## need to load all the data
path <- "C:\\Users\\rosemaryt\\ShareFile\\Personal Folders\\reproducing short term forecasting papers\\day ahead forecasts_v2\\zone"
#zones <- c(1:10)
zone <- 1
q_list <- seq(0.05, 0.95,0.05) # the quantiles we want to forecast for
## parameter options to explore
searchGridSubCol <- expand.grid(shrinkage = seq(0.01, 0.1, 0.03),interaction.depth = seq(2, 12, 3))
n_param_combs <- dim(searchGridSubCol)[1] # number of parameter combinations we need to iterate over.
## load formula and var_mono
load(paste0(path, zone, "fcinputs.rda"))
## load training data
load(paste0(path, zone, "traindata.rda"))
## add on all the q50 forecasts
load(paste0(path, "s_q50_fcs.Rda"))
zdata <- merge(zdata, forecast_df, by=c('timestamp', 'kfold'))
## and remember to add them into FORMULA too...
not_fc_inputs <- c("timestamp", "kfold") # specify what columns in q50 forecasts are NOT forecast inputs
q50_inputs <- setdiff(names(forecast_df), not_fc_inputs)
existing_inputs <- labels(terms(FORMULA))
all_inputs <- c(existing_inputs, q50_inputs)
formula2 <- as.formula(paste0("TARGETVAR ~ ",paste0(all_inputs,collapse = " + ")))
## if we want to select from all possible forecast inputs (rather than using only those we found via feature selection in 1st stage)
# not_fc_inputs <- c("ZONEID", "TARGETVAR", "timestamp", "kfold") # easier to specify what columns are NOT forecast inputs!
# fc_inputs <- setdiff(names(training_data), not_fc_inputs)
# formula_new <- as.formula(paste0("TARGETVAR ~ ",paste0(fc_inputs,collapse = " + ")))
training_data <- zdata[kfold != 'Test']
foldnames <- unique(training_data$kfold)
ALL_forecasts <- data.table() # empty data table to put forecasts in as we loop thorugh fold combinations.
##############################################################################################
########################################## functions #########################################
##############################################################################################
find_opt_params <- function(input_data,fmla, mono_var, shrinkage_vec,interaction_vec,
quantiles, n_trees = 1000L,ncores= NULL,...){
## find the optimal shrinkage and interation depth.
## input data must be already WITHOUT the fold you finally want forecasts for...
if(length(shrinkage_vec)!=length(interaction_vec)){
stop("length(shrinkage_vec))!=length(interaction_vec")
}
if(is.null(ncores)){ncores <- length(quantiles)}
mqr_cv <- lapply(seq_along(shrinkage_vec),function(x){
preds <- MQR_gbm(data = input_data,
formula = fmla,
quantiles = quantiles,
gbm_params = list(shrinkage = shrinkage_vec[x],
interaction.depth = interaction_vec[x],
n.trees = n_trees,
var.monotone=mono_var),
parallel = T,
cores = ncores,
pckgs = c("data.table"),
perf.plot=F,
Sort = T,
SortLimits = list(U=1, L=0),
pred_ntree = n_trees)
rel <- data.table(reliability(qrdata = preds, realisations = input_data[,get(as.character(fmla[[2]]))],plot.it = F))
pball <- data.table(pinball(qrdata = preds, realisations = input_data[,get(as.character(fmla[[2]]))],plot.it = F))
return(list(shrink = shrinkage_vec[x],
inter = interaction_vec[x],
reliab = rel,
pinb = pball))
})
cv_results <- lapply(mqr_cv,as.data.table)
cv_results <- rbindlist(cv_results)
# average pinball loss for each run and price region
cv_avpb <- cv_results[,list(av_pb=mean(pinb.Loss)),by=.(shrink,inter)]
# find minimum shrinkage and depth combo
cv_avpb <- cv_avpb[,.SD[which.min(av_pb)]] # just keep the row with the best (average) pinball loss
return (cv_avpb)
}
## make sure both sin and cos terms are included if just one is (both either hod or moy):
func_pairseason <- function(x){
if("sin_moy"%in%x){
if(sum("cos_moy"%in%x)==0){
x <- c(x,"cos_moy")
}
}
if("cos_moy"%in%x){
if(sum("sin_moy"%in%x)==0){
x <- c(x,"sin_moy")
}
}
return(x)
}
func_pairhours <- function(x){
if("sin_hod"%in%x){
if(sum("cos_hod"%in%x)==0){
x <- c(x,"cos_hod")
}
}
if("cos_hod"%in%x){
if(sum("sin_hod"%in%x)==0){
x <- c(x,"sin_hod")
}
}
return(x)
}
##############################################################################################
## Find optimal GBM parameters by cross validation on the training data
## First, get feature importances
## by training some really sparse regression trees to get feature importances for three key quantiles
vars <- list()
for(j in c(0.1,0.5,0.9)){
mod <- gbm(formula = formula2,
distribution = list(name = "quantile", alpha = j),
data = training_data,
n.trees = 1000L,
interaction.depth = 2,
shrinkage = 0.01,
n.minobsinnode=30, # minimum number obs per leaf
bag.fraction = 0.9, # fraction of obervations used for next tree
keep.data =F,
verbose=F)
tmp <- data.table(summary(mod,n.trees = 1000L, plotit = F))
vars[[paste0("q",j)]] <- as.character(tmp[rel.inf>=1]$var)
rm(tmp,mod)
}
unique_vars <- unique(unlist(vars)) # get unique variables at each location
unique_vars <- func_pairseason(unique_vars) # add in the other sin/cos moy term if only one in unique_vars already
unique_vars <- func_pairhours(unique_vars)  # same for sin/cos hod
final_formula <- as.formula(paste0("TARGETVAR ~ ",paste0(unique_vars,collapse = " + ")))
var_mono <- rep(0,length(unique_vars))
var_mono[which(unique_vars %in% c("sin_moy","cos_moy","sin_hod","cos_hod"))] <- -1 # set var_monotone to -1 for the circular (sin/cos) terms
## find optimal shirnkage and interaction depth (~50 mins)
modelparams <- find_opt_params(training_data, final_formula, var_mono,
searchGridSubCol$shrinkage, searchGridSubCol$interaction.depth, quantiles=q_list)
## RUN UP TO HERE
## now fit on numbered folds (training) and generate forecasts for test fold, using optimal parameters
gbm_mqr <- MQR_gbm(data = zdata,
formula = final_formula,
quantiles = q_list,
gbm_params = list(interaction.depth = modelparams$inter,
n.trees = 1000,
shrinkage = modelparams$shrink,
n.minobsinnode = 30,
bag.fraction = 0.5,
keep.data = F,
var.monotone = var_mono),
parallel = T,
cores = detectCores(),
Sort=T)
gbm_mqr <- as.data.table(gbm_mqr)
forecasts <- cbind(zdata[,c("timestamp", "kfold")], gbm_mqr)
save(forecasts, file=paste0(path, zone, "final_fcs.Rda"))
print (modelparams)
print (Sys.time() - start_time)
zones <- c(2:9)
save(modelparams, file=paste0(path, zone, "modelparams.Rda"))
fwrite(modelparams, file=paste0(path, zone, "modelparams.csv"))
require(data.table)
require(graphics)
require(ProbCast)
"Uses features deemed important from the q50 feature importance,
plus all the zones' q50 forecasts, and does feature importance again on these."
## need to load all the data
path <- "C:\\Users\\rosemaryt\\ShareFile\\Personal Folders\\reproducing short term forecasting papers\\day ahead forecasts_v2\\zone"
zones <- c(2:9)
# zone <- 1
q_list <- seq(0.05, 0.95,0.05) # the quantiles we want to forecast for
##############################################################################################
########################################## functions #########################################
##############################################################################################
find_opt_params <- function(input_data,fmla, mono_var, shrinkage_vec,interaction_vec,
quantiles, n_trees = 1000L,ncores= NULL,...){
## find the optimal shrinkage and interation depth.
## input data must be already WITHOUT the fold you finally want forecasts for...
if(length(shrinkage_vec)!=length(interaction_vec)){
stop("length(shrinkage_vec))!=length(interaction_vec")
}
if(is.null(ncores)){ncores <- length(quantiles)}
mqr_cv <- lapply(seq_along(shrinkage_vec),function(x){
preds <- MQR_gbm(data = input_data,
formula = fmla,
quantiles = quantiles,
gbm_params = list(shrinkage = shrinkage_vec[x],
interaction.depth = interaction_vec[x],
n.trees = n_trees,
var.monotone=mono_var),
parallel = T,
cores = ncores,
pckgs = c("data.table"),
perf.plot=F,
Sort = T,
SortLimits = list(U=1, L=0),
pred_ntree = n_trees)
rel <- data.table(reliability(qrdata = preds, realisations = input_data[,get(as.character(fmla[[2]]))],plot.it = F))
pball <- data.table(pinball(qrdata = preds, realisations = input_data[,get(as.character(fmla[[2]]))],plot.it = F))
return(list(shrink = shrinkage_vec[x],
inter = interaction_vec[x],
reliab = rel,
pinb = pball))
})
cv_results <- lapply(mqr_cv,as.data.table)
cv_results <- rbindlist(cv_results)
# average pinball loss for each run and price region
cv_avpb <- cv_results[,list(av_pb=mean(pinb.Loss)),by=.(shrink,inter)]
# find minimum shrinkage and depth combo
cv_avpb <- cv_avpb[,.SD[which.min(av_pb)]] # just keep the row with the best (average) pinball loss
return (cv_avpb)
}
## make sure both sin and cos terms are included if just one is (both either hod or moy):
func_pairseason <- function(x){
if("sin_moy"%in%x){
if(sum("cos_moy"%in%x)==0){
x <- c(x,"cos_moy")
}
}
if("cos_moy"%in%x){
if(sum("sin_moy"%in%x)==0){
x <- c(x,"sin_moy")
}
}
return(x)
}
func_pairhours <- function(x){
if("sin_hod"%in%x){
if(sum("cos_hod"%in%x)==0){
x <- c(x,"cos_hod")
}
}
if("cos_hod"%in%x){
if(sum("sin_hod"%in%x)==0){
x <- c(x,"sin_hod")
}
}
return(x)
}
##############################################################################################
for (zone in zones){
print (zone)
zstart_time <- Sys.time()
## parameter options to explore
searchGridSubCol <- expand.grid(shrinkage = seq(0.01, 0.1, 0.03),interaction.depth = seq(2, 12, 3))
n_param_combs <- dim(searchGridSubCol)[1] # number of parameter combinations we need to iterate over.
## load formula and var_mono
load(paste0(path, zone, "fcinputs.rda"))
## load training data
load(paste0(path, zone, "traindata.rda"))
## add on all the q50 forecasts
load(paste0(path, "s_q50_fcs.Rda"))
zdata <- merge(zdata, forecast_df, by=c('timestamp', 'kfold'))
## and remember to add them into FORMULA too...
not_fc_inputs <- c("timestamp", "kfold") # specify what columns in q50 forecasts are NOT forecast inputs
q50_inputs <- setdiff(names(forecast_df), not_fc_inputs)
existing_inputs <- labels(terms(FORMULA))
all_inputs <- c(existing_inputs, q50_inputs)
formula2 <- as.formula(paste0("TARGETVAR ~ ",paste0(all_inputs,collapse = " + ")))
## if we want to select from all possible forecast inputs (rather than using only those we found via feature selection in 1st stage)
# not_fc_inputs <- c("ZONEID", "TARGETVAR", "timestamp", "kfold") # easier to specify what columns are NOT forecast inputs!
# fc_inputs <- setdiff(names(training_data), not_fc_inputs)
# formula_new <- as.formula(paste0("TARGETVAR ~ ",paste0(fc_inputs,collapse = " + ")))
training_data <- zdata[kfold != 'Test']
## Find optimal GBM parameters by cross validation on the training data
## First, get feature importances
## by training some really sparse regression trees to get feature importances for three key quantiles
vars <- list()
for(j in c(0.1,0.5,0.9)){
mod <- gbm(formula = formula2,
distribution = list(name = "quantile", alpha = j),
data = training_data,
n.trees = 1000L,
interaction.depth = 2,
shrinkage = 0.01,
n.minobsinnode=30, # minimum number obs per leaf
bag.fraction = 0.9, # fraction of obervations used for next tree
keep.data =F,
verbose=F)
tmp <- data.table(summary(mod,n.trees = 1000L, plotit = F))
vars[[paste0("q",j)]] <- as.character(tmp[rel.inf>=1]$var)
rm(tmp,mod)
}
unique_vars <- unique(unlist(vars)) # get unique variables at each location
unique_vars <- func_pairseason(unique_vars) # add in the other sin/cos moy term if only one in unique_vars already
unique_vars <- func_pairhours(unique_vars)  # same for sin/cos hod
final_formula <- as.formula(paste0("TARGETVAR ~ ",paste0(unique_vars,collapse = " + ")))
var_mono <- rep(0,length(unique_vars))
var_mono[which(unique_vars %in% c("sin_moy","cos_moy","sin_hod","cos_hod"))] <- -1 # set var_monotone to -1 for the circular (sin/cos) terms
## find optimal shirnkage and interaction depth (~50 mins)
modelparams <- find_opt_params(training_data, final_formula, var_mono,
searchGridSubCol$shrinkage, searchGridSubCol$interaction.depth, quantiles=q_list)
## RUN UP TO HERE
## now fit on numbered folds (training) and generate forecasts for test fold, using optimal parameters
gbm_mqr <- MQR_gbm(data = zdata,
formula = final_formula,
quantiles = q_list,
gbm_params = list(interaction.depth = modelparams$inter,
n.trees = 1000,
shrinkage = modelparams$shrink,
n.minobsinnode = 30,
bag.fraction = 0.5,
keep.data = F,
var.monotone = var_mono),
parallel = T,
cores = detectCores(),
Sort=T)
gbm_mqr <- as.data.table(gbm_mqr)
forecasts <- cbind(zdata[,c("timestamp", "kfold")], gbm_mqr)
save(forecasts, file=paste0(path, zone, "final_fcs.Rda"))
fwrite(modelparams, file=paste0(path, zone, "modelparams.csv"))
#print (Sys.time() - zstart_time)
}
# zone <- 1
short_qs <- seq(0.1, 0.9, 0.1)
require(data.table)
require(graphics)
require(ProbCast)
"Uses features deemed important from the q50 feature importance,
plus all the zones' q50 forecasts, and does feature importance again on these."
## need to load all the data
path <- "C:\\Users\\rosemaryt\\ShareFile\\Personal Folders\\reproducing short term forecasting papers\\day ahead forecasts_v2\\zone"
zones <- c(1:10)
# zone <- 1
short_qs <- seq(0.1, 0.9, 0.1)
q_list <- seq(0.05, 0.95,0.05) # the quantiles we want to forecast for
##############################################################################################
########################################## functions #########################################
##############################################################################################
find_opt_params <- function(input_data,fmla, mono_var, shrinkage_vec,interaction_vec,
quantiles, n_trees = 1000L,ncores= NULL,...){
## find the optimal shrinkage and interation depth.
## input data must be already WITHOUT the fold you finally want forecasts for...
if(length(shrinkage_vec)!=length(interaction_vec)){
stop("length(shrinkage_vec))!=length(interaction_vec")
}
if(is.null(ncores)){ncores <- length(quantiles)}
mqr_cv <- lapply(seq_along(shrinkage_vec),function(x){
preds <- MQR_gbm(data = input_data,
formula = fmla,
quantiles = quantiles,
gbm_params = list(shrinkage = shrinkage_vec[x],
interaction.depth = interaction_vec[x],
n.trees = n_trees,
var.monotone=mono_var),
parallel = T,
cores = ncores,
pckgs = c("data.table"),
perf.plot=F,
Sort = T,
SortLimits = list(U=1, L=0),
pred_ntree = n_trees)
rel <- data.table(reliability(qrdata = preds, realisations = input_data[,get(as.character(fmla[[2]]))],plot.it = F))
pball <- data.table(pinball(qrdata = preds, realisations = input_data[,get(as.character(fmla[[2]]))],plot.it = F))
return(list(shrink = shrinkage_vec[x],
inter = interaction_vec[x],
reliab = rel,
pinb = pball))
})
cv_results <- lapply(mqr_cv,as.data.table)
cv_results <- rbindlist(cv_results)
# average pinball loss for each run and price region
cv_avpb <- cv_results[,list(av_pb=mean(pinb.Loss)),by=.(shrink,inter)]
# find minimum shrinkage and depth combo
cv_avpb <- cv_avpb[,.SD[which.min(av_pb)]] # just keep the row with the best (average) pinball loss
return (cv_avpb)
}
## make sure both sin and cos terms are included if just one is (both either hod or moy):
func_pairseason <- function(x){
if("sin_moy"%in%x){
if(sum("cos_moy"%in%x)==0){
x <- c(x,"cos_moy")
}
}
if("cos_moy"%in%x){
if(sum("sin_moy"%in%x)==0){
x <- c(x,"sin_moy")
}
}
return(x)
}
func_pairhours <- function(x){
if("sin_hod"%in%x){
if(sum("cos_hod"%in%x)==0){
x <- c(x,"cos_hod")
}
}
if("cos_hod"%in%x){
if(sum("sin_hod"%in%x)==0){
x <- c(x,"sin_hod")
}
}
return(x)
}
##############################################################################################
for (zone in zones){
print (zone)
zstart_time <- Sys.time()
## parameter options to explore
searchGridSubCol <- expand.grid(shrinkage = seq(0.01, 0.1, 0.03),interaction.depth = seq(2, 12, 3))
n_param_combs <- dim(searchGridSubCol)[1] # number of parameter combinations we need to iterate over.
## load formula and var_mono
load(paste0(path, zone, "fcinputs.rda"))
## load training data
load(paste0(path, zone, "traindata.rda"))
## add on all the q50 forecasts
load(paste0(path, "s_q50_fcs.Rda"))
zdata <- merge(zdata, forecast_df, by=c('timestamp', 'kfold'))
## and remember to add them into FORMULA too...
not_fc_inputs <- c("timestamp", "kfold") # specify what columns in q50 forecasts are NOT forecast inputs
q50_inputs <- setdiff(names(forecast_df), not_fc_inputs)
existing_inputs <- labels(terms(FORMULA))
all_inputs <- c(existing_inputs, q50_inputs)
formula2 <- as.formula(paste0("TARGETVAR ~ ",paste0(all_inputs,collapse = " + ")))
## if we want to select from all possible forecast inputs (rather than using only those we found via feature selection in 1st stage)
# not_fc_inputs <- c("ZONEID", "TARGETVAR", "timestamp", "kfold") # easier to specify what columns are NOT forecast inputs!
# fc_inputs <- setdiff(names(training_data), not_fc_inputs)
# formula_new <- as.formula(paste0("TARGETVAR ~ ",paste0(fc_inputs,collapse = " + ")))
training_data <- zdata[kfold != 'Test']
## Find optimal GBM parameters by cross validation on the training data
## First, get feature importances
## by training some really sparse regression trees to get feature importances for three key quantiles
vars <- list()
for(j in c(0.1,0.5,0.9)){
mod <- gbm(formula = formula2,
distribution = list(name = "quantile", alpha = j),
data = training_data,
n.trees = 1000L,
interaction.depth = 2,
shrinkage = 0.01,
n.minobsinnode=30, # minimum number obs per leaf
bag.fraction = 0.9, # fraction of obervations used for next tree
keep.data =F,
verbose=F)
tmp <- data.table(summary(mod,n.trees = 1000L, plotit = F))
vars[[paste0("q",j)]] <- as.character(tmp[rel.inf>=1]$var)
rm(tmp,mod)
}
unique_vars <- unique(unlist(vars)) # get unique variables at each location
unique_vars <- func_pairseason(unique_vars) # add in the other sin/cos moy term if only one in unique_vars already
unique_vars <- func_pairhours(unique_vars)  # same for sin/cos hod
final_formula <- as.formula(paste0("TARGETVAR ~ ",paste0(unique_vars,collapse = " + ")))
var_mono <- rep(0,length(unique_vars))
var_mono[which(unique_vars %in% c("sin_moy","cos_moy","sin_hod","cos_hod"))] <- -1 # set var_monotone to -1 for the circular (sin/cos) terms
## find optimal shirnkage and interaction depth (~50 mins)
modelparams <- find_opt_params(training_data, final_formula, var_mono,
searchGridSubCol$shrinkage, searchGridSubCol$interaction.depth, quantiles=short_qs)
## now fit on numbered folds (training) and generate forecasts for test fold, using optimal parameters
gbm_mqr <- MQR_gbm(data = zdata,
formula = final_formula,
quantiles = q_list,
gbm_params = list(interaction.depth = modelparams$inter,
n.trees = 1000,
shrinkage = modelparams$shrink,
n.minobsinnode = 30,
bag.fraction = 0.5,
keep.data = F,
var.monotone = var_mono),
parallel = T,
cores = detectCores(),
Sort=T)
gbm_mqr <- as.data.table(gbm_mqr)
forecasts <- cbind(zdata[,c("timestamp", "kfold")], gbm_mqr)
save(forecasts, file=paste0(path, zone, "final_fcs.Rda"))
fwrite(modelparams, file=paste0(path, zone, "modelparams.csv"))
#print (Sys.time() - zstart_time)
}
remove.packages("lightgbm")
install.packages("lightgbm", repos = "https://cran.r-project.org")
### first of all, the CRAN example....
library(lightgbm)
install.packages("lightgbm", repos = "https://cran.r-project.org")
remove.packages('lightgbm')
install.packages(c("bitops", "brio", "callr", "caTools", "cli", "cluster", "desc", "devtools", "diffobj", "dplyr", "DT", "gert", "gh", "highr", "httpuv", "isoband", "knitr", "later", "mboost", "mgcv", "pillar", "pkgload", "processx", "RcppArmadillo", "recipes", "remotes", "rgl", "survival", "tibble", "utf8", "vctrs", "viridisLite", "waldo", "withr", "xfun"))
remove.packages(lightgbm)
remove.packages("lightgbm")
remove.packages("ProbCast")
require(devtools)
install_github("jbrowell/Probcast")
install.packages("lightgbm")
require(data.table)
require(ProbCast)
require(rstudioapi)
setwd(dirname(getActiveDocumentContext()$path))
zone <- 1
horizons <- c(1:6)
models <- c("Persistence", "VAR", "MC", "EMD")
Nboots <- 500 # number of times to bootstrap resample
## load all the different model forecasts for this zone
load(file=paste0("../persistence/zone", zone, "_forecasts.rda"))
load(file=paste0("../VAR/final_quantiles_zone",zone,".rda"))
## how about just the constant variance model??
# load(file=paste0("../VAR/zone",zone,"_qs_constvar.rda"))
# load(file=paste0("../VAR/zone",zone,"_qs_expsmoothvar.rda"))
load(file=paste0("../MC/test_forecasts_zone",zone,".rda"))
load(file=paste0("../EMD/final_quantiles_zone",zone,".rda"))
get_reliability <- function(forecasts, horizon, timestamps, nb, model){
## timestamps are the target times we want to evaluate for
rowinds <- forecasts[[2]][(Horizon==horizon) & (target_time %in% timestamps), which=TRUE]
rel <- reliability(qrdata=forecasts[[1]][rowinds,], realisations = forecasts[[2]][rowinds, ActualPower], bootstrap=nb, plot.it=T)
rel <- as.data.table(rel)
rel[, flat_empirical := Empirical-Nominal]
rel[, flat_upper := upper-Nominal]
rel[, flat_lower := lower-Nominal]
rel[, Model := model]
return(rel)
}
for (horizon in horizons){
times1 <- merge(persistence_fcs[[2]][Horizon==horizon, .(issue_time, target_time)], VAR_fcs[[2]][Horizon==horizon, .(issue_time, target_time)], by=c('issue_time', 'target_time'))
times2 <- merge(MC_fcs[[2]][Horizon==horizon, .(issue_time, target_time)], EMD_fcs[[2]][Horizon==horizon, .(issue_time, target_time)])
mutual_times <- merge(times1, times2)
p_rel <- get_reliability(persistence_fcs, horizon, mutual_times$target_time, Nboots, "Persistence")
var_rel <- get_reliability(VAR_fcs, horizon, mutual_times$target_time, Nboots, "VAR")
mc_rel <- get_reliability(MC_fcs, horizon, mutual_times$target_time, Nboots, "MC")
emd_rel <- get_reliability(EMD_fcs, horizon, mutual_times$target_time, Nboots, "EMD")
reliability_dt <- rbind(p_rel, var_rel, mc_rel, emd_rel)
fwrite(reliability_dt, file=paste0("./zone",zone, "_h",horizon,"_reliability.csv"))
}
